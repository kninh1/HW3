---
title: "Assignment3"
author: "Katrina Ninh"
format:
  html:
    embed-resources: true
---

## Text Mining

A new dataset has been added to the data science data repository <https://github.com/USCbiostats/data-science-data/tree/master/03_pubmed>. The dataset contains 3,241 abstracts from articles collected via 5 PubMed searches. The search terms are listed in the second column, `term` and these will serve as the \"documents.\" Your job is to analyse these abstracts to find interesting insights.

1.  Tokenize the abstracts and count the number of each token. Do you see anything interesting? Does removing stop words change what tokens appear as the most frequent? What are the 5 most common tokens for each search term after removing stopwords?

2.  Tokenize the abstracts into bigrams. Find the 10 most common bigrams and visualize them with ggplot2.

3.  Calculate the TF-IDF value for each word-search term combination (here you want the search term to be the \"document\"). What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?

Load libraries & file

```{r}

library(dplyr)
library(tidyverse)
library(ggplot2)
library(tidytext)
library(leaflet)

if (!file.exists("pubmed.csv"))
  download.file(
    url = "https://raw.githubusercontent.com/USCbiostats/data-science-data/master/03_pubmed/pubmed.csv",
    destfile = "pubmed.csv",
    method   = "libcurl",
    timeout  = 60
    )
dat <- read.csv("pubmed.csv")

nrow(dat)
ncol(dat)

```

## Question 1

1.  Tokenize the abstracts and count the number of each token. Do you see anything interesting? Does removing stop words change what tokens appear as the most frequent? What are the 5 most common tokens for each search term after removing stopwords?

```{r}

# Tokenizing the abstracts
words <- dat %>%
  unnest_tokens(token, abstract)

words %>%
  count(token, sort = TRUE) %>%
  top_n(10, n)

```

There are a lot of stop words before the keyword "covid" - let's remove them

```{r}

## Removing stop words

# Group by "term" and count tokens for each search term
words %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  group_by(term) %>%
  count(token, sort=TRUE) %>%
  top_n(10, n)
  

unique(words$term)

```

After removing the stop words, the five remaining most frequent words are: "covid", "19", "cancer", "prostate", and "patients".

## Question 2

Tokenize the abstracts into bigrams. Find the 10 most common bigrams and visualize them with ggplot2

```{r}

# Tokenizing the abstracts into bigrams
dat %>%
  unnest_ngrams(ngram, abstract, n = 2) %>%
  count(ngram, sort = TRUE) %>%
  top_n(20, n) 

```

At this point, we still see stop words like "of the" and "in the", let's remove them as well.

```{r}

datClean <- dat %>%
  unnest_ngrams(word, abstract ,n=2) %>%
  separate (word, c("word1", "word2"),sep = " ") %>%
  anti_join(
    tidytext::stop_words, by =c("word1" = "word")
  ) %>%
  anti_join(
    tidytext::stop_words, by =c("word2" = "word")
  ) %>%

  unite(dat2, word1, word2, sep= " ")
datClean %>%
count(dat2, sort = T) %>%
 top_n(10,n) %>%
 ggplot(aes(n, fct_reorder(dat2, n)))+
 geom_col()


```

The 10 most common bigrams as shown above: "covid 19", "prostate cancer", etc.

## Question 3

Calculate the TF-IDF value for each word-search term combination (here you want the search term to be the \"document\"). What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?

```{r}

datTF <- dat %>%
  unnest_tokens(abstract, abstract) %>% 
  filter(!(abstract %in% stop_words$word)) %>%
  count(abstract, term) %>%
  group_by(term)%>%
  bind_tf_idf(abstract, term, n) %>%
  top_n(5, n) %>%
  arrange(desc(tf_idf))
datTF %>%
  group_by(term) %>%
  arrange(desc(tf_idf))

unique(datTF$term)


```

In question 1, after removing the stop words, the five remaining most frequent words are: covid, 19, patients, cancer, and prostate.

In question 3, the results are more meaningful. Since the TF-IDF value measures how relevant a word is to a document, the terms arranged by highest TF-IDF "covid", "prostate cancer", "preeclampsia", "meningitis", "cystic fibrosis", gives us more details of each terms.
